{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:39:53.348806Z",
     "start_time": "2018-05-15T05:39:53.345777Z"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.937012Z",
     "start_time": "2018-05-15T07:25:29.984509Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from helpers import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "# Use the Azure Machine Learning data preparation package\n",
    "# from azureml.dataprep import package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.945018Z",
     "start_time": "2018-05-15T07:25:40.939016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Supported datasets:\n",
    "# impact\n",
    "# urgency\n",
    "column_to_predict = \"urgency\"\n",
    "text_columns = \"title\"\n",
    "# Supported algorithms\n",
    "# \"SVM\"\n",
    "# \"NB\"\n",
    "# \"KNN\"\n",
    "# \"D3\"\n",
    "classifier = \"SVM\"\n",
    "\n",
    "# Grid Search is used to find hyperparameters\n",
    "# Searching for hyperparameters is time consuming\n",
    "use_grid_search = True\n",
    "\n",
    "# Removes stop words from processed text\n",
    "remove_stop_words = True\n",
    "\n",
    "# Used with 'remove_stop_words' and defines language of stop words collection\n",
    "stop_words_lang = 'english'\n",
    "\n",
    "# Word stemming using NLTK\n",
    "use_stemming = False\n",
    "\n",
    "# If use_stemming == True, fit_prior should be set to False\n",
    "fit_prior = True\n",
    "\n",
    "# min_data_per_class is used to determine number of samples required for each class.\n",
    "# Classes with less than that will be excluded from the dataset\n",
    "# Default value is 1\n",
    "min_data_per_class = 1  \n",
    "\n",
    "# Supported samplers\n",
    "# \"over\"\n",
    "# \"smote\"\n",
    "sampler = \"smote\"\n",
    "\n",
    "# KNN cannot be used with Grid Search\n",
    "if classifier == \"KNN\":\n",
    "    use_grid_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.218329Z",
     "start_time": "2018-05-15T07:25:40.947014Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading dataset from dprep in Workbench    \n",
    "# dfTickets = package.run('AllTickets.dprep', dataflow_idx=0) \n",
    "\n",
    "# loading dataset from csv\n",
    "dfTickets = pd.read_csv(\n",
    "    './datasets/all_tickets_2.csv',\n",
    "    engine='python',\n",
    "    dtype=str\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Poorly Represented Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.321758Z",
     "start_time": "2018-05-15T07:25:41.220101Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removing classes with less then 1 rows: (1576, 5)\n",
      "Number of classes before removing classes with less then 1 rows: 3\n",
      "Shape of dataset after removing classes with less then 1 rows: (1576, 5)\n",
      "Number of classes after removing classes with less then 1 rows: 3\n"
     ]
    }
   ],
   "source": [
    "# Removing rows related to classes represented by low amount of data\n",
    "print(\"Shape of dataset before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(dfTickets.shape))\n",
    "print(\"Number of classes before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(len(np.unique(dfTickets[column_to_predict]))))\n",
    "bytag = dfTickets.groupby(column_to_predict).aggregate(np.count_nonzero)\n",
    "tags = bytag[bytag.body > min_data_per_class].index\n",
    "dfTickets = dfTickets[dfTickets[column_to_predict].isin(tags)]\n",
    "print( \"Shape of dataset after removing classes with less then \" + str(min_data_per_class) + \" rows: \" + str(dfTickets.shape))\n",
    "print(\"Number of classes after removing classes with less then \" + str(min_data_per_class) + \" rows: \" + str(len(np.unique(dfTickets[column_to_predict]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset into Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.365785Z",
     "start_time": "2018-05-15T07:25:41.324755Z"
    }
   },
   "outputs": [],
   "source": [
    "labelData = dfTickets[column_to_predict]\n",
    "data = dfTickets[text_columns]\n",
    "\n",
    "# Split dataset into training and testing data with 80:20 ratio\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labelData, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features from the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.374133Z",
     "start_time": "2018-05-15T07:25:41.368126Z"
    }
   },
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:42.972298Z",
     "start_time": "2018-05-15T07:25:41.377130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 1066)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if remove_stop_words:\n",
    "    count_vect = CountVectorizer(stop_words=stop_words_lang)\n",
    "elif use_stemming:\n",
    "    count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n",
    "else:\n",
    "    count_vect = CountVectorizer()\n",
    "\n",
    "vectorized_data = count_vect.fit_transform(train_data)\n",
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:43.026267Z",
     "start_time": "2018-05-15T07:25:42.975265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 1066)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "features = tfidf.fit_transform(vectorized_data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Imbalance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampler == \"over\":\n",
    "    imbl_samp = RandomOverSampler()\n",
    "else:\n",
    "    imbl_samp = SMOTE(random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use A Pipeline to Preprocess Data and Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:44.786016Z",
     "start_time": "2018-05-15T07:25:43.028264Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM classifier\n"
     ]
    }
   ],
   "source": [
    "if classifier == \"NB\":\n",
    "    print(\"Training NB classifier\")\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "#         ('samp', imbl_samp),\n",
    "        ('clf', MultinomialNB(fit_prior=fit_prior))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"D3\":\n",
    "    print(\"Training D3 classifier\")\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "#         ('samp', imbl_samp),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"SVM\":\n",
    "    print(\"Training SVM classifier\")\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "#         ('samp', imbl_samp),\n",
    "        ('clf', SVC(kernel='linear'))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"KNN\":\n",
    "    print(\"Training KNN classifier\")\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "#         ('samp', imbl_samp),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors = 3))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_clf, open('./pickle/xtremax/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GridSearchCV to Find the Best Params Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.271405Z",
     "start_time": "2018-05-15T07:25:44.789019Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of parameters for performance tuning\n",
    "if use_grid_search:\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__use_idf': (True, False)\n",
    "    }\n",
    "    # Create GS instance by passing the classifier, parameters and n_jobs=-1 (use multiple cores from user machine)\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "\n",
    "    # To see the best mean score and the params\n",
    "    gs_clf.best_score_\n",
    "    gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.280680Z",
     "start_time": "2018-05-15T07:26:20.274677Z"
    }
   },
   "source": [
    "# Save GSCV Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    pickle.dump(text_clf, open('./pickle/xtremax/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.189731Z",
     "start_time": "2018-05-15T07:26:20.282676Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "Confusion matrix without GridSearch:\n",
      "[[  8   7  31]\n",
      " [  4  20  47]\n",
      " [  0  22 177]]\n",
      "Mean without GridSearch: 0.6487341772151899\n",
      "Confusion matrix with GridSearch:\n",
      "[[  8   8  30]\n",
      " [  4  17  50]\n",
      " [  0  25 174]]\n",
      "Mean with GridSearch: 0.629746835443038\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model\")\n",
    "# Score and evaluate model on test data using model without hyperparameter tuning\n",
    "predicted = text_clf.predict(test_data)\n",
    "prediction_acc = np.mean(predicted == test_labels)\n",
    "print(\"Confusion matrix without GridSearch:\")\n",
    "print(metrics.confusion_matrix(test_labels, predicted))\n",
    "print(\"Mean without GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "\n",
    "# Score and evaluate model on test data using model with hyperparameter tuning\n",
    "if use_grid_search:\n",
    "    predicted = gs_clf.predict(test_data)\n",
    "    prediction_acc = np.mean(predicted == test_labels)\n",
    "    print(\"Confusion matrix with GridSearch:\")\n",
    "    print(metrics.confusion_matrix(test_labels, predicted))\n",
    "    print(\"Mean with GridSearch: \" + str(prediction_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model with Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestTickets = pd.read_csv(\n",
    "    './test data/testing_tickets.csv',\n",
    "    dtype=str\n",
    ")\n",
    "test_input_data = dfTestTickets[text_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict without GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "title ,\n",
      "service unavailable\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 3\n",
      "title ,\n",
      "for us not working\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 4\n",
      "title ,\n",
      "not responding\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 5\n",
      "title ,\n",
      "performance issues and\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 6\n",
      "title ,\n",
      "connection down th floor\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 7\n",
      "title ,\n",
      "inaccessible\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 8\n",
      "title ,\n",
      "urgent cannot open files on\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 9\n",
      "title ,\n",
      "wireless network down\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 10\n",
      "title ,\n",
      "not accessible\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 11\n",
      "title ,\n",
      "certificate expired\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 12\n",
      "title ,\n",
      "oracle errors when logging\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 13\n",
      "title ,\n",
      "ad account change\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 14\n",
      "title ,\n",
      "conference codes needed\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 15\n",
      "title ,\n",
      "account for\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 16\n",
      "title ,\n",
      "conference line for\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 17\n",
      "title ,\n",
      "dial codes\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 18\n",
      "title ,\n",
      "dial details\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 19\n",
      "title ,\n",
      "provide new code\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 20\n",
      "title ,\n",
      "file\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 21\n",
      "title ,\n",
      "cannot access\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 22\n",
      "title ,\n",
      "unable to connect to\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 23\n",
      "title ,\n",
      "issue\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 24\n",
      "title ,\n",
      "investigate why open issue keeps occurring every day\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 25\n",
      "title ,\n",
      "error\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 26\n",
      "title ,\n",
      "open issue\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 27\n",
      "title ,\n",
      "is down can access the network\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 28\n",
      "title ,\n",
      "open\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 29\n",
      "title ,\n",
      "provost issues\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 30\n",
      "title ,\n",
      "problems connecting to\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 31\n",
      "title ,\n",
      "access to\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "text_clf_model = pickle.load(open('./pickle/xtremax/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "prediction_input = text_clf_model.predict(test_input_data)\n",
    "\n",
    "i = 0\n",
    "for result in prediction_input:\n",
    "    print(\"No:\",i+2)\n",
    "    print(text_columns,\",\")\n",
    "    print(test_input_data.iloc[i])\n",
    "    print(\"-\"*30)\n",
    "    print(column_to_predict,\":\", result)\n",
    "    print(\"=\"*30)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "title ,\n",
      "service unavailable\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 3\n",
      "title ,\n",
      "for us not working\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 4\n",
      "title ,\n",
      "not responding\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 5\n",
      "title ,\n",
      "performance issues and\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 6\n",
      "title ,\n",
      "connection down th floor\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 7\n",
      "title ,\n",
      "inaccessible\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 8\n",
      "title ,\n",
      "urgent cannot open files on\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 9\n",
      "title ,\n",
      "wireless network down\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 10\n",
      "title ,\n",
      "not accessible\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 11\n",
      "title ,\n",
      "certificate expired\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 12\n",
      "title ,\n",
      "oracle errors when logging\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 13\n",
      "title ,\n",
      "ad account change\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 14\n",
      "title ,\n",
      "conference codes needed\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 15\n",
      "title ,\n",
      "account for\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 16\n",
      "title ,\n",
      "conference line for\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 17\n",
      "title ,\n",
      "dial codes\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 18\n",
      "title ,\n",
      "dial details\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 19\n",
      "title ,\n",
      "provide new code\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 20\n",
      "title ,\n",
      "file\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 21\n",
      "title ,\n",
      "cannot access\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 22\n",
      "title ,\n",
      "unable to connect to\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 23\n",
      "title ,\n",
      "issue\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 24\n",
      "title ,\n",
      "investigate why open issue keeps occurring every day\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 25\n",
      "title ,\n",
      "error\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 26\n",
      "title ,\n",
      "open issue\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 27\n",
      "title ,\n",
      "is down can access the network\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 28\n",
      "title ,\n",
      "open\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 29\n",
      "title ,\n",
      "provost issues\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 30\n",
      "title ,\n",
      "problems connecting to\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 31\n",
      "title ,\n",
      "access to\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "if use_grid_search:\n",
    "    gs_clf_model = pickle.load(open('./pickle/xtremax/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "    prediction_input = gs_clf_model.predict(test_input_data)\n",
    "\n",
    "    i = 0\n",
    "    for result in prediction_input:\n",
    "        print(\"No:\",i+2)\n",
    "        print(text_columns,\",\")\n",
    "        print(test_input_data.iloc[i])\n",
    "        print(\"-\"*30)\n",
    "        print(column_to_predict,\":\", result)\n",
    "        print(\"=\"*30)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.627104Z",
     "start_time": "2018-05-15T07:26:21.192694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting confusion matrix with 'seaborn' module\n",
    "# Use below line only with Jupyter Notebook\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "mat = confusion_matrix(test_labels, predicted)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set()\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(test_labels),\n",
    "            yticklabels=np.unique(test_labels))\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "# Save confusion matrix to outputs in Workbench\n",
    "# plt.savefig(os.path.join('.', 'outputs', 'confusion_matrix.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing the Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.684170Z",
     "start_time": "2018-05-15T07:26:21.631088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.17      0.28        46\n",
      "           2       0.34      0.24      0.28        71\n",
      "           3       0.69      0.87      0.77       199\n",
      "\n",
      "    accuracy                           0.63       316\n",
      "   macro avg       0.56      0.43      0.44       316\n",
      "weighted avg       0.60      0.63      0.59       316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, predicted,\n",
    "                            target_names=np.unique(test_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
