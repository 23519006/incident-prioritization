{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:39:53.348806Z",
     "start_time": "2018-05-15T05:39:53.345777Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.937012Z",
     "start_time": "2018-05-15T07:25:29.984509Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from helpers import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "# Use the Azure Machine Learning data preparation package\n",
    "# from azureml.dataprep import package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.945018Z",
     "start_time": "2018-05-15T07:25:40.939016Z"
    }
   },
   "outputs": [],
   "source": [
    "column_to_predict = \"impact\"\n",
    "# Supported datasets:\n",
    "# ticket_type\n",
    "# business_service\n",
    "# category\n",
    "# impact\n",
    "# urgency\n",
    "# sub_category1\n",
    "# sub_category2\n",
    "\n",
    "classifier = \"NB\"  # Supported algorithms # \"SVM\" # \"NB\"\n",
    "use_grid_search = True  # grid search is used to find hyperparameters. Searching for hyperparameters is time consuming\n",
    "remove_stop_words = True  # removes stop words from processed text\n",
    "stop_words_lang = 'english'  # used with 'remove_stop_words' and defines language of stop words collection\n",
    "use_stemming = False  # word stemming using nltk\n",
    "fit_prior = True  # if use_stemming == True then it should be set to False ?? double check\n",
    "min_data_per_class = 1  # used to determine number of samples required for each class.Classes with less than that will be excluded from the dataset. default value is 1\n",
    "\n",
    "\n",
    "sampler = \"Over\"\n",
    "\n",
    "if classifier == \"KNN\":\n",
    "    use_grid_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.218329Z",
     "start_time": "2018-05-15T07:25:40.947014Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading dataset from dprep in Workbench    \n",
    "# dfTickets = package.run('AllTickets.dprep', dataflow_idx=0) \n",
    "\n",
    "# loading dataset from csv\n",
    "dfTickets = pd.read_csv(\n",
    "    './datasets/all_tickets.csv',\n",
    "    dtype=str\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select 'TEXT' column and remove poorly represented classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.321758Z",
     "start_time": "2018-05-15T07:25:41.220101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes before removing classes with less then 1 rows: 3\n",
      "Shape of dataset after removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes after removing classes with less then 1 rows: 3\n"
     ]
    }
   ],
   "source": [
    "text_columns = \"business_service\"  # \"title\" - text columns used for TF-IDF\n",
    "\n",
    "# Removing rows related to classes represented by low amount of data\n",
    "print(\"Shape of dataset before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(dfTickets.shape))\n",
    "print(\"Number of classes before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(len(np.unique(dfTickets[column_to_predict]))))\n",
    "bytag = dfTickets.groupby(column_to_predict).aggregate(np.count_nonzero)\n",
    "tags = bytag[bytag.body > min_data_per_class].index\n",
    "dfTickets = dfTickets[dfTickets[column_to_predict].isin(tags)]\n",
    "print(\n",
    "    \"Shape of dataset after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(dfTickets.shape)\n",
    ")\n",
    "print(\n",
    "    \"Number of classes after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(len(np.unique(dfTickets[column_to_predict])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and labels and split them to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.365785Z",
     "start_time": "2018-05-15T07:25:41.324755Z"
    }
   },
   "outputs": [],
   "source": [
    "labelData = dfTickets[column_to_predict]\n",
    "data = dfTickets[text_columns]\n",
    "\n",
    "# Split dataset into training and testing data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labelData, test_size=0.2\n",
    ")  # split data to train/test sets with 80:20 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.374133Z",
     "start_time": "2018-05-15T07:25:41.368126Z"
    }
   },
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:42.972298Z",
     "start_time": "2018-05-15T07:25:41.377130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 90)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorizer\n",
    "if remove_stop_words:\n",
    "    count_vect = CountVectorizer(stop_words=stop_words_lang)\n",
    "elif use_stemming:\n",
    "    count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n",
    "else:\n",
    "    count_vect = CountVectorizer()\n",
    "\n",
    "vectorized_data = count_vect.fit_transform(train_data)\n",
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:43.026267Z",
     "start_time": "2018-05-15T07:25:42.975265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 90)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "features = tfidf.fit_transform(vectorized_data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampler == \"Over\":\n",
    "    imbl_samp = RandomOverSampler()\n",
    "elif sampler == \"Under\":\n",
    "    imbl_samp = RandomUnderSampler()\n",
    "else:\n",
    "    imbl_samp = RandomOverSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipeline to preprocess data and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:44.786016Z",
     "start_time": "2018-05-15T07:25:43.028264Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NB classifier\n"
     ]
    }
   ],
   "source": [
    "# Fitting the training data into a data processing pipeline and eventually into the model itself\n",
    "if classifier == \"NB\":\n",
    "    print(\"Training NB classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', MultinomialNB(fit_prior=fit_prior))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"D3\":\n",
    "    print(\"Training D3 classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"SVM\":\n",
    "    print(\"Training SVM classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', SVC(kernel='linear'))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"KNN\":\n",
    "    print(\"Training KNN classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors = 3))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_clf, open('./pickle/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GridSearchCV to search for best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.271405Z",
     "start_time": "2018-05-15T07:25:44.789019Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    # Grid Search\n",
    "    # Here, we are creating a list of parameters for which we would like to do performance tuning.\n",
    "    # All the parameters name start with the classifier name (remember the arbitrary name we gave).\n",
    "    # E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "    # NB parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__use_idf': (True, False)\n",
    "    }\n",
    "\n",
    "    # Next, we create an instance of the grid search by passing the classifier, parameters\n",
    "    # and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "    \n",
    "\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "\n",
    "    # To see the best mean score and the params, run the following code\n",
    "    gs_clf.best_score_\n",
    "    gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.280680Z",
     "start_time": "2018-05-15T07:26:20.274677Z"
    }
   },
   "source": [
    "# Save GSCV Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    pickle.dump(text_clf, open('./pickle/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.189731Z",
     "start_time": "2018-05-15T07:26:20.282676Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "Confusion matrix without GridSearch:\n",
      "[[  17    6   12]\n",
      " [ 517  874  715]\n",
      " [1631  982 3868]]\n",
      "Mean without GridSearch: 0.5519601020644862\n",
      "Confusion matrix with GridSearch:\n",
      "[[  17    6   12]\n",
      " [ 505  870  731]\n",
      " [1495  975 4011]]\n",
      "Mean with GridSearch: 0.5680816515889585\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model\")\n",
    "# Score and evaluate model on test data using model without hyperparameter tuning\n",
    "predicted = text_clf.predict(test_data)\n",
    "prediction_acc = np.mean(predicted == test_labels)\n",
    "print(\"Confusion matrix without GridSearch:\")\n",
    "print(metrics.confusion_matrix(test_labels, predicted))\n",
    "print(\"Mean without GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "\n",
    "# Score and evaluate model on test data using model WITH hyperparameter tuning\n",
    "if use_grid_search:\n",
    "    predicted = gs_clf.predict(test_data)\n",
    "    prediction_acc = np.mean(predicted == test_labels)\n",
    "    print(\"Confusion matrix with GridSearch:\")\n",
    "    print(metrics.confusion_matrix(test_labels, predicted))\n",
    "    print(\"Mean with GridSearch: \" + str(prediction_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data with inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestTickets = pd.read_csv(\n",
    "    './test data/testing_tickets.csv',\n",
    "    dtype=str\n",
    ")\n",
    "test_input_data = dfTestTickets[text_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "business_service ,\n",
      "19\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 3\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 4\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 5\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 6\n",
      "business_service ,\n",
      "45\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 7\n",
      "business_service ,\n",
      "50\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 8\n",
      "business_service ,\n",
      "61\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 9\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 10\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 11\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 12\n",
      "business_service ,\n",
      "67\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 13\n",
      "business_service ,\n",
      "1\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 14\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 15\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 16\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 17\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 18\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 19\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 20\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 21\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 22\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 23\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 24\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 25\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 26\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 27\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 28\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 29\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 30\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 31\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "text_clf_model = pickle.load(open('./pickle/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "prediction_input = text_clf_model.predict(test_input_data)\n",
    "\n",
    "i = 0\n",
    "for result in prediction_input:\n",
    "    print(\"No:\",i+2)\n",
    "    print(text_columns,\",\")\n",
    "    print(test_input_data.iloc[i])\n",
    "    print(\"-\"*30)\n",
    "    print(column_to_predict,\":\", result)\n",
    "    print(\"=\"*30)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "business_service ,\n",
      "19\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 3\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 4\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 5\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 6\n",
      "business_service ,\n",
      "45\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 7\n",
      "business_service ,\n",
      "50\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 8\n",
      "business_service ,\n",
      "61\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 9\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 10\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 11\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 12\n",
      "business_service ,\n",
      "67\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 13\n",
      "business_service ,\n",
      "1\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 14\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 15\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 16\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 17\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 18\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 19\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 20\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 21\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 22\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 23\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 24\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 25\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 26\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 27\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 28\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 29\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 30\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 31\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "if use_grid_search:\n",
    "    gs_clf_model = pickle.load(open('./pickle/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "    prediction_input = gs_clf_model.predict(test_input_data)\n",
    "\n",
    "    i = 0\n",
    "    for result in prediction_input:\n",
    "        print(\"No:\",i+2)\n",
    "        print(text_columns,\",\")\n",
    "        print(test_input_data.iloc[i])\n",
    "        print(\"-\"*30)\n",
    "        print(column_to_predict,\":\", result)\n",
    "        print(\"=\"*30)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.627104Z",
     "start_time": "2018-05-15T07:26:21.192694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting confusion matrix with 'seaborn' module\n",
    "# Use below line only with Jupyter Notebook\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "mat = confusion_matrix(test_labels, predicted)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set()\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(test_labels),\n",
    "            yticklabels=np.unique(test_labels))\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "# Save confusion matrix to outputs in Workbench\n",
    "# plt.savefig(os.path.join('.', 'outputs', 'confusion_matrix.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.684170Z",
     "start_time": "2018-05-15T07:26:21.631088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.01      0.49      0.02        35\n",
      "           2       0.47      0.41      0.44      2106\n",
      "           3       0.84      0.62      0.71      6481\n",
      "\n",
      "    accuracy                           0.57      8622\n",
      "   macro avg       0.44      0.51      0.39      8622\n",
      "weighted avg       0.75      0.57      0.64      8622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, predicted,\n",
    "                            target_names=np.unique(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
