{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import DecisionTreeClassifier\n",
    "from nltk import MaxentClassifier\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "# sb = SnowballStemmer('english')\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'us',\n",
    "              'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
    "              'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself',\n",
    "              'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "              'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having',\n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "              'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "              'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
    "              'from', 'again', 'further', 'then', 'might', 'must', 'need', 'shall', 'once',\n",
    "              'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few',\n",
    "              'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than',\n",
    "              'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now',\n",
    "              'd', 'll', 'm', 'o', 're', 've', 'y', 'ain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTickets = pd.read_csv(\"all_tickets.csv\",dtype=str)\n",
    "# print(data[['title']])\n",
    "print(dfTickets['title'])\n",
    "# tickets_title = data[['title']]\n",
    "# tickets_body = data[['body']]\n",
    "# category = data[['category']]\n",
    "# business_service = data[['business_service']]\n",
    "# urgency = data[['urgency']]\n",
    "# impact = data[['impact']]\n",
    "# priority = data[['priority']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize + Lemmatize Data + Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"title length:\",len(tickets_title))\n",
    "preprocessed_title = []\n",
    "for i in range(0,43106):\n",
    "    lemmatized = []\n",
    "    word = tickets_title.loc[i,'title'].lower()\n",
    "    tokenizeWord = word_tokenize(word)\n",
    "    filtered_sentence = [w for w in tokenizeWord if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in tokenizeWord: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)         \n",
    "\n",
    "    for word in filtered_sentence:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word))\n",
    "        \n",
    "    preprocessed_title.append(lemmatized)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(preprocessed_title[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Body Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"body length:\",len(tickets_body))\n",
    "preprocessed_body = []\n",
    "for i in range(0,43106):\n",
    "    lemmatized = []\n",
    "    word = tickets_body.loc[i,'body'].lower()\n",
    "    tokenizeWord = word_tokenize(word)\n",
    "    filtered_sentence = [w for w in tokenizeWord if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in tokenizeWord: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)         \n",
    "\n",
    "    for word in filtered_sentence:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word))\n",
    "        \n",
    "    preprocessed_body.append(lemmatized)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(preprocessed_body[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_predict = \"urgency\"\n",
    "text_columns = \"title\"\n",
    "data = dfTickets[[text_columns]]\n",
    "labelData = dfTickets[[column_to_predict]]\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labelData, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Set Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title / Urgency Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = StemmedCountVectorizer(stop_words=\"english\")\n",
    "vectorized_data = count_vect.fit_transform(train_data)\n",
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "features = tfidf.fit_transform(vectorized_data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB(fit_prior=True))\n",
    "    ])\n",
    "text_clf = text_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model\")\n",
    "# Score and evaluate model on test data using model without hyperparameter tuning\n",
    "predicted = text_clf.predict(test_data)\n",
    "prediction_acc = np.mean(predicted == test_labels)\n",
    "print(\"Confusion matrix without GridSearch:\")\n",
    "print(metrics.confusion_matrix(test_labels, predicted))\n",
    "print(\"Mean without GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "# Score and evaluate model on test data using model WITH hyperparameter tuning\n",
    "if use_grid_search:\n",
    "    predicted = gs_clf.predict(test_data)\n",
    "    prediction_acc = np.mean(predicted == test_labels)\n",
    "    print(\"Confusion matrix with GridSearch:\")\n",
    "    print(metrics.confusion_matrix(test_labels, predicted))\n",
    "    print(\"Mean with GridSearch: \" + str(prediction_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting confusion matrix with 'seaborn' module\n",
    "# Use below line only with Jupyter Notebook\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "mat = confusion_matrix(test_labels, predicted)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set()\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(test_labels),\n",
    "            yticklabels=np.unique(test_labels))\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "# Save confusion matrix to outputs in Workbench\n",
    "# plt.savefig(os.path.join('.', 'outputs', 'confusion_matrix.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, predicted,\n",
    "                            target_names=np.unique(test_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
