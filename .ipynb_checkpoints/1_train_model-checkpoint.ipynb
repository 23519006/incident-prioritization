{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:39:53.348806Z",
     "start_time": "2018-05-15T05:39:53.345777Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.937012Z",
     "start_time": "2018-05-15T07:25:29.984509Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from helpers import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "# Use the Azure Machine Learning data preparation package\n",
    "# from azureml.dataprep import package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.945018Z",
     "start_time": "2018-05-15T07:25:40.939016Z"
    }
   },
   "outputs": [],
   "source": [
    "column_to_predict = \"impact\"\n",
    "# Supported datasets:\n",
    "# ticket_type\n",
    "# business_service\n",
    "# category\n",
    "# impact\n",
    "# urgency\n",
    "# sub_category1\n",
    "# sub_category2\n",
    "\n",
    "classifier = \"NB\"  # Supported algorithms # \"SVM\" # \"NB\"\n",
    "use_grid_search = True  # grid search is used to find hyperparameters. Searching for hyperparameters is time consuming\n",
    "remove_stop_words = True  # removes stop words from processed text\n",
    "stop_words_lang = 'english'  # used with 'remove_stop_words' and defines language of stop words collection\n",
    "use_stemming = False  # word stemming using nltk\n",
    "fit_prior = True  # if use_stemming == True then it should be set to False ?? double check\n",
    "min_data_per_class = 1  # used to determine number of samples required for each class.Classes with less than that will be excluded from the dataset. default value is 1\n",
    "\n",
    "\n",
    "sampler = \"smote\"\n",
    "\n",
    "if classifier == \"KNN\":\n",
    "    use_grid_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.218329Z",
     "start_time": "2018-05-15T07:25:40.947014Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading dataset from dprep in Workbench    \n",
    "# dfTickets = package.run('AllTickets.dprep', dataflow_idx=0) \n",
    "\n",
    "# loading dataset from csv\n",
    "dfTickets = pd.read_csv(\n",
    "    './datasets/all_tickets.csv',\n",
    "    dtype=str\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select 'TEXT' column and remove poorly represented classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.321758Z",
     "start_time": "2018-05-15T07:25:41.220101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes before removing classes with less then 1 rows: 3\n",
      "Shape of dataset after removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes after removing classes with less then 1 rows: 3\n"
     ]
    }
   ],
   "source": [
    "text_columns = \"business_service\"  # \"title\" - text columns used for TF-IDF\n",
    "\n",
    "# Removing rows related to classes represented by low amount of data\n",
    "print(\"Shape of dataset before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(dfTickets.shape))\n",
    "print(\"Number of classes before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(len(np.unique(dfTickets[column_to_predict]))))\n",
    "bytag = dfTickets.groupby(column_to_predict).aggregate(np.count_nonzero)\n",
    "tags = bytag[bytag.body > min_data_per_class].index\n",
    "dfTickets = dfTickets[dfTickets[column_to_predict].isin(tags)]\n",
    "print(\n",
    "    \"Shape of dataset after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(dfTickets.shape)\n",
    ")\n",
    "print(\n",
    "    \"Number of classes after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(len(np.unique(dfTickets[column_to_predict])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and labels and split them to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.365785Z",
     "start_time": "2018-05-15T07:25:41.324755Z"
    }
   },
   "outputs": [],
   "source": [
    "labelData = dfTickets[column_to_predict]\n",
    "data = dfTickets[text_columns]\n",
    "\n",
    "# Split dataset into training and testing data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labelData, test_size=0.2\n",
    ")  # split data to train/test sets with 80:20 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.374133Z",
     "start_time": "2018-05-15T07:25:41.368126Z"
    }
   },
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:42.972298Z",
     "start_time": "2018-05-15T07:25:41.377130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 91)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorizer\n",
    "if remove_stop_words:\n",
    "    count_vect = CountVectorizer(stop_words=stop_words_lang)\n",
    "elif use_stemming:\n",
    "    count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n",
    "else:\n",
    "    count_vect = CountVectorizer()\n",
    "\n",
    "vectorized_data = count_vect.fit_transform(train_data)\n",
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:43.026267Z",
     "start_time": "2018-05-15T07:25:42.975265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 91)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "features = tfidf.fit_transform(vectorized_data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampler == \"Over\":\n",
    "    imbl_samp = RandomOverSampler()\n",
    "else:\n",
    "    imbl_samp = SMOTE(random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipeline to preprocess data and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:44.786016Z",
     "start_time": "2018-05-15T07:25:43.028264Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NB classifier\n"
     ]
    }
   ],
   "source": [
    "# Fitting the training data into a data processing pipeline and eventually into the model itself\n",
    "if classifier == \"NB\":\n",
    "    print(\"Training NB classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', MultinomialNB(fit_prior=fit_prior))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"D3\":\n",
    "    print(\"Training D3 classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"SVM\":\n",
    "    print(\"Training SVM classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', SVC(kernel='linear'))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"KNN\":\n",
    "    print(\"Training KNN classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors = 3))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_clf, open('./pickle/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GridSearchCV to search for best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.271405Z",
     "start_time": "2018-05-15T07:25:44.789019Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    # Grid Search\n",
    "    # Here, we are creating a list of parameters for which we would like to do performance tuning.\n",
    "    # All the parameters name start with the classifier name (remember the arbitrary name we gave).\n",
    "    # E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "    # NB parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__use_idf': (True, False)\n",
    "    }\n",
    "\n",
    "    # Next, we create an instance of the grid search by passing the classifier, parameters\n",
    "    # and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "    \n",
    "\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "\n",
    "    # To see the best mean score and the params, run the following code\n",
    "    gs_clf.best_score_\n",
    "    gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.280680Z",
     "start_time": "2018-05-15T07:26:20.274677Z"
    }
   },
   "source": [
    "# Save GSCV Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    pickle.dump(text_clf, open('./pickle/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.189731Z",
     "start_time": "2018-05-15T07:26:20.282676Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "Confusion matrix without GridSearch:\n",
      "[[  25   13   17]\n",
      " [ 598  817  735]\n",
      " [1561  802 4054]]\n",
      "Mean without GridSearch: 0.5678496868475992\n",
      "Confusion matrix with GridSearch:\n",
      "[[  25   13   17]\n",
      " [ 598  817  735]\n",
      " [1561  802 4054]]\n",
      "Mean with GridSearch: 0.5678496868475992\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model\")\n",
    "# Score and evaluate model on test data using model without hyperparameter tuning\n",
    "predicted = text_clf.predict(test_data)\n",
    "prediction_acc = np.mean(predicted == test_labels)\n",
    "print(\"Confusion matrix without GridSearch:\")\n",
    "print(metrics.confusion_matrix(test_labels, predicted))\n",
    "print(\"Mean without GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "\n",
    "# Score and evaluate model on test data using model WITH hyperparameter tuning\n",
    "if use_grid_search:\n",
    "    predicted = gs_clf.predict(test_data)\n",
    "    prediction_acc = np.mean(predicted == test_labels)\n",
    "    print(\"Confusion matrix with GridSearch:\")\n",
    "    print(metrics.confusion_matrix(test_labels, predicted))\n",
    "    print(\"Mean with GridSearch: \" + str(prediction_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data with inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestTickets = pd.read_csv(\n",
    "    './test data/testing_tickets.csv',\n",
    "    dtype=str\n",
    ")\n",
    "test_input_data = dfTestTickets[text_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "business_service ,\n",
      "19\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 3\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 4\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 5\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 6\n",
      "business_service ,\n",
      "45\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 7\n",
      "business_service ,\n",
      "50\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 8\n",
      "business_service ,\n",
      "61\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 9\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 10\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 11\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 12\n",
      "business_service ,\n",
      "67\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 13\n",
      "business_service ,\n",
      "1\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 14\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 15\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 16\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 17\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 18\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 19\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 20\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 21\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 22\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 23\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 24\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 25\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 26\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 27\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 28\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 29\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 30\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 31\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "text_clf_model = pickle.load(open('./pickle/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "prediction_input = text_clf_model.predict(test_input_data)\n",
    "\n",
    "i = 0\n",
    "for result in prediction_input:\n",
    "    print(\"No:\",i+2)\n",
    "    print(text_columns,\",\")\n",
    "    print(test_input_data.iloc[i])\n",
    "    print(\"-\"*30)\n",
    "    print(column_to_predict,\":\", result)\n",
    "    print(\"=\"*30)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "business_service ,\n",
      "19\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 3\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 4\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 5\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 6\n",
      "business_service ,\n",
      "45\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 7\n",
      "business_service ,\n",
      "50\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 8\n",
      "business_service ,\n",
      "61\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 9\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 10\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 11\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 12\n",
      "business_service ,\n",
      "67\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 13\n",
      "business_service ,\n",
      "1\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 14\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 15\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 16\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 17\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 18\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 19\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 20\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 21\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 22\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 23\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 24\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 25\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 26\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 27\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 28\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 29\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 30\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 31\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "if use_grid_search:\n",
    "    gs_clf_model = pickle.load(open('./pickle/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "    prediction_input = gs_clf_model.predict(test_input_data)\n",
    "\n",
    "    i = 0\n",
    "    for result in prediction_input:\n",
    "        print(\"No:\",i+2)\n",
    "        print(text_columns,\",\")\n",
    "        print(test_input_data.iloc[i])\n",
    "        print(\"-\"*30)\n",
    "        print(column_to_predict,\":\", result)\n",
    "        print(\"=\"*30)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.627104Z",
     "start_time": "2018-05-15T07:26:21.192694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting confusion matrix with 'seaborn' module\n",
    "# Use below line only with Jupyter Notebook\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "mat = confusion_matrix(test_labels, predicted)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set()\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(test_labels),\n",
    "            yticklabels=np.unique(test_labels))\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "# Save confusion matrix to outputs in Workbench\n",
    "# plt.savefig(os.path.join('.', 'outputs', 'confusion_matrix.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.684170Z",
     "start_time": "2018-05-15T07:26:21.631088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.01      0.45      0.02        55\n",
      "           2       0.50      0.38      0.43      2150\n",
      "           3       0.84      0.63      0.72      6417\n",
      "\n",
      "    accuracy                           0.57      8622\n",
      "   macro avg       0.45      0.49      0.39      8622\n",
      "weighted avg       0.75      0.57      0.65      8622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, predicted,\n",
    "                            target_names=np.unique(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
