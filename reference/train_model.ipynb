{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:39:53.348806Z",
     "start_time": "2018-05-15T05:39:53.345777Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.937012Z",
     "start_time": "2018-05-15T07:25:29.984509Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from helpers import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "# Use the Azure Machine Learning data preparation package\n",
    "# from azureml.dataprep import package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.945018Z",
     "start_time": "2018-05-15T07:25:40.939016Z"
    }
   },
   "outputs": [],
   "source": [
    "column_to_predict = \"urgency\"\n",
    "# Supported datasets:\n",
    "# ticket_type\n",
    "# business_service\n",
    "# category\n",
    "# impact\n",
    "# urgency\n",
    "# sub_category1\n",
    "# sub_category2\n",
    "\n",
    "classifier = \"SVM\"  # Supported algorithms # \"SVM\" # \"NB\"\n",
    "use_grid_search = False  # grid search is used to find hyperparameters. Searching for hyperparameters is time consuming\n",
    "remove_stop_words = True  # removes stop words from processed text\n",
    "stop_words_lang = 'english'  # used with 'remove_stop_words' and defines language of stop words collection\n",
    "use_stemming = False  # word stemming using nltk\n",
    "fit_prior = True  # if use_stemming == True then it should be set to False ?? double check\n",
    "min_data_per_class = 1  # used to determine number of samples required for each class.Classes with less than that will be excluded from the dataset. default value is 1\n",
    "\n",
    "\n",
    "sampler = \"Over\"\n",
    "\n",
    "if classifier == \"KNN\":\n",
    "    use_grid_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.218329Z",
     "start_time": "2018-05-15T07:25:40.947014Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading dataset from dprep in Workbench    \n",
    "# dfTickets = package.run('AllTickets.dprep', dataflow_idx=0) \n",
    "\n",
    "# loading dataset from csv\n",
    "dfTickets = pd.read_csv(\n",
    "    './datasets/all_tickets.csv',\n",
    "    dtype=str\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select 'TEXT' column and remove poorly represented classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.321758Z",
     "start_time": "2018-05-15T07:25:41.220101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes before removing classes with less then 1 rows: 3\n",
      "Shape of dataset after removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes after removing classes with less then 1 rows: 3\n"
     ]
    }
   ],
   "source": [
    "text_columns = \"category\"  # \"title\" - text columns used for TF-IDF\n",
    "\n",
    "# Removing rows related to classes represented by low amount of data\n",
    "print(\"Shape of dataset before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(dfTickets.shape))\n",
    "print(\"Number of classes before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(len(np.unique(dfTickets[column_to_predict]))))\n",
    "bytag = dfTickets.groupby(column_to_predict).aggregate(np.count_nonzero)\n",
    "tags = bytag[bytag.body > min_data_per_class].index\n",
    "dfTickets = dfTickets[dfTickets[column_to_predict].isin(tags)]\n",
    "print(\n",
    "    \"Shape of dataset after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(dfTickets.shape)\n",
    ")\n",
    "print(\n",
    "    \"Number of classes after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(len(np.unique(dfTickets[column_to_predict])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and labels and split them to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.365785Z",
     "start_time": "2018-05-15T07:25:41.324755Z"
    }
   },
   "outputs": [],
   "source": [
    "labelData = dfTickets[column_to_predict]\n",
    "data = dfTickets[text_columns]\n",
    "\n",
    "# Split dataset into training and testing data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labelData, test_size=0.2\n",
    ")  # split data to train/test sets with 80:20 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.374133Z",
     "start_time": "2018-05-15T07:25:41.368126Z"
    }
   },
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:42.972298Z",
     "start_time": "2018-05-15T07:25:41.377130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorizer\n",
    "if remove_stop_words:\n",
    "    count_vect = CountVectorizer(stop_words=stop_words_lang)\n",
    "elif use_stemming:\n",
    "    count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n",
    "else:\n",
    "    count_vect = CountVectorizer()\n",
    "\n",
    "vectorized_data = count_vect.fit_transform(train_data)\n",
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:43.026267Z",
     "start_time": "2018-05-15T07:25:42.975265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "features = tfidf.fit_transform(vectorized_data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampler == \"Over\":\n",
    "    imbl_samp = RandomOverSampler()\n",
    "elif sampler == \"Under\":\n",
    "    imbl_samp = RandomUnderSampler()\n",
    "else:\n",
    "    imbl_samp = RandomOverSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipeline to preprocess data and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:44.786016Z",
     "start_time": "2018-05-15T07:25:43.028264Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM classifier\n"
     ]
    }
   ],
   "source": [
    "# Fitting the training data into a data processing pipeline and eventually into the model itself\n",
    "if classifier == \"NB\":\n",
    "    print(\"Training NB classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', MultinomialNB(fit_prior=fit_prior))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"D3\":\n",
    "    print(\"Training D3 classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"SVM\":\n",
    "    print(\"Training SVM classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', SVC(kernel='linear'))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"KNN\":\n",
    "    print(\"Training KNN classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors = 3))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_clf, open('./pickle/text_clf_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GridSearchCV to search for best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.271405Z",
     "start_time": "2018-05-15T07:25:44.789019Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    # Grid Search\n",
    "    # Here, we are creating a list of parameters for which we would like to do performance tuning.\n",
    "    # All the parameters name start with the classifier name (remember the arbitrary name we gave).\n",
    "    # E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "    # NB parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__use_idf': (True, False)\n",
    "    }\n",
    "\n",
    "    # Next, we create an instance of the grid search by passing the classifier, parameters\n",
    "    # and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "    \n",
    "\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "\n",
    "    # To see the best mean score and the params, run the following code\n",
    "    gs_clf.best_score_\n",
    "    gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.280680Z",
     "start_time": "2018-05-15T07:26:20.274677Z"
    }
   },
   "source": [
    "# Save GSCV Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_clf, open('./pickle/gs_clf_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.189731Z",
     "start_time": "2018-05-15T07:26:20.282676Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "Confusion matrix without GridSearch:\n",
      "[[   0 1187    0]\n",
      " [   0  993    0]\n",
      " [   0 6433    9]]\n",
      "Mean without GridSearch: 0.116214335421016\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model\")\n",
    "# Score and evaluate model on test data using model without hyperparameter tuning\n",
    "predicted = text_clf.predict(test_data)\n",
    "prediction_acc = np.mean(predicted == test_labels)\n",
    "print(\"Confusion matrix without GridSearch:\")\n",
    "print(metrics.confusion_matrix(test_labels, predicted))\n",
    "print(\"Mean without GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "\n",
    "# Score and evaluate model on test data using model WITH hyperparameter tuning\n",
    "if use_grid_search:\n",
    "    predicted = gs_clf.predict(test_data)\n",
    "    prediction_acc = np.mean(predicted == test_labels)\n",
    "    print(\"Confusion matrix with GridSearch:\")\n",
    "    print(metrics.confusion_matrix(test_labels, predicted))\n",
    "    print(\"Mean with GridSearch: \" + str(prediction_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data with inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestTickets = pd.read_csv(\n",
    "    './test data/testing_tickets.csv',\n",
    "    dtype=str\n",
    ")\n",
    "test_input_data = dfTestTickets[text_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 3\n",
      "category ,\n",
      "6\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 4\n",
      "category ,\n",
      "6\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 5\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 6\n",
      "category ,\n",
      "6\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 7\n",
      "category ,\n",
      "6\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 8\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 9\n",
      "category ,\n",
      "6\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 10\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 11\n",
      "category ,\n",
      "5\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 12\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 13\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 14\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 15\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 16\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 17\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 18\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 19\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 20\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 21\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 22\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 23\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 24\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 25\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 26\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 27\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 28\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 29\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 30\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 31\n",
      "category ,\n",
      "4\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "text_clf_model = pickle.load(open('./pickle/text_clf_model.pickle',\"rb\"))\n",
    "prediction_input = text_clf_model.predict(test_input_data)\n",
    "\n",
    "i = 0\n",
    "for result in prediction_input:\n",
    "    print(\"No:\",i+2)\n",
    "    print(text_columns,\",\")\n",
    "    print(test_input_data.iloc[i])\n",
    "    print(\"-\"*30)\n",
    "    print(column_to_predict,\":\", result)\n",
    "    print(\"=\"*30)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    prediction_input = gs_clf.predict(test_input_data)\n",
    "\n",
    "    i = 0\n",
    "    for result in prediction_input:\n",
    "        print(\"No:\",i+2)\n",
    "        print(text_columns,\",\")\n",
    "        print(test_input_data.iloc[i])\n",
    "        print(\"-\"*30)\n",
    "        print(column_to_predict,\":\", result)\n",
    "        print(\"=\"*30)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.627104Z",
     "start_time": "2018-05-15T07:26:21.192694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting confusion matrix with 'seaborn' module\n",
    "# Use below line only with Jupyter Notebook\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "mat = confusion_matrix(test_labels, predicted)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set()\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(test_labels),\n",
    "            yticklabels=np.unique(test_labels))\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "# Save confusion matrix to outputs in Workbench\n",
    "# plt.savefig(os.path.join('.', 'outputs', 'confusion_matrix.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.684170Z",
     "start_time": "2018-05-15T07:26:21.631088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00      1187\n",
      "           2       0.12      1.00      0.21       993\n",
      "           3       1.00      0.00      0.00      6442\n",
      "\n",
      "    accuracy                           0.12      8622\n",
      "   macro avg       0.37      0.33      0.07      8622\n",
      "weighted avg       0.76      0.12      0.03      8622\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tessa angela\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, predicted,\n",
    "                            target_names=np.unique(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
