{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:39:53.348806Z",
     "start_time": "2018-05-15T05:39:53.345777Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.937012Z",
     "start_time": "2018-05-15T07:25:29.984509Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from helpers import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "# Use the Azure Machine Learning data preparation package\n",
    "# from azureml.dataprep import package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.945018Z",
     "start_time": "2018-05-15T07:25:40.939016Z"
    }
   },
   "outputs": [],
   "source": [
    "column_to_predict = \"urgency\"\n",
    "# Supported datasets:\n",
    "# ticket_type\n",
    "# business_service\n",
    "# category\n",
    "# impact\n",
    "# urgency\n",
    "# sub_category1\n",
    "# sub_category2\n",
    "\n",
    "classifier = \"KNN\"  # Supported algorithms # \"SVM\" # \"NB\"\n",
    "use_grid_search = True  # grid search is used to find hyperparameters. Searching for hyperparameters is time consuming\n",
    "remove_stop_words = True  # removes stop words from processed text\n",
    "stop_words_lang = 'english'  # used with 'remove_stop_words' and defines language of stop words collection\n",
    "use_stemming = False  # word stemming using nltk\n",
    "fit_prior = True  # if use_stemming == True then it should be set to False ?? double check\n",
    "min_data_per_class = 1  # used to determine number of samples required for each class.Classes with less than that will be excluded from the dataset. default value is 1\n",
    "\n",
    "\n",
    "sampler = \"Over\"\n",
    "\n",
    "if classifier == \"KNN\":\n",
    "    use_grid_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.218329Z",
     "start_time": "2018-05-15T07:25:40.947014Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading dataset from dprep in Workbench    \n",
    "# dfTickets = package.run('AllTickets.dprep', dataflow_idx=0) \n",
    "\n",
    "# loading dataset from csv\n",
    "dfTickets = pd.read_csv(\n",
    "    './datasets/all_tickets.csv',\n",
    "    dtype=str\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select 'TEXT' column and remove poorly represented classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.321758Z",
     "start_time": "2018-05-15T07:25:41.220101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes before removing classes with less then 1 rows: 3\n",
      "Shape of dataset after removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes after removing classes with less then 1 rows: 3\n"
     ]
    }
   ],
   "source": [
    "text_columns = \"business_service\"  # \"title\" - text columns used for TF-IDF\n",
    "\n",
    "# Removing rows related to classes represented by low amount of data\n",
    "print(\"Shape of dataset before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(dfTickets.shape))\n",
    "print(\"Number of classes before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(len(np.unique(dfTickets[column_to_predict]))))\n",
    "bytag = dfTickets.groupby(column_to_predict).aggregate(np.count_nonzero)\n",
    "tags = bytag[bytag.body > min_data_per_class].index\n",
    "dfTickets = dfTickets[dfTickets[column_to_predict].isin(tags)]\n",
    "print(\n",
    "    \"Shape of dataset after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(dfTickets.shape)\n",
    ")\n",
    "print(\n",
    "    \"Number of classes after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(len(np.unique(dfTickets[column_to_predict])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and labels and split them to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.365785Z",
     "start_time": "2018-05-15T07:25:41.324755Z"
    }
   },
   "outputs": [],
   "source": [
    "labelData = dfTickets[column_to_predict]\n",
    "data = dfTickets[text_columns]\n",
    "\n",
    "# Split dataset into training and testing data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labelData, test_size=0.2\n",
    ")  # split data to train/test sets with 80:20 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.374133Z",
     "start_time": "2018-05-15T07:25:41.368126Z"
    }
   },
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:42.972298Z",
     "start_time": "2018-05-15T07:25:41.377130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 91)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorizer\n",
    "if remove_stop_words:\n",
    "    count_vect = CountVectorizer(stop_words=stop_words_lang)\n",
    "elif use_stemming:\n",
    "    count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n",
    "else:\n",
    "    count_vect = CountVectorizer()\n",
    "\n",
    "vectorized_data = count_vect.fit_transform(train_data)\n",
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:43.026267Z",
     "start_time": "2018-05-15T07:25:42.975265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 91)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "features = tfidf.fit_transform(vectorized_data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampler == \"Over\":\n",
    "    imbl_samp = RandomOverSampler()\n",
    "elif sampler == \"Under\":\n",
    "    imbl_samp = RandomUnderSampler()\n",
    "else:\n",
    "    imbl_samp = RandomOverSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipeline to preprocess data and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:44.786016Z",
     "start_time": "2018-05-15T07:25:43.028264Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN classifier\n"
     ]
    }
   ],
   "source": [
    "# Fitting the training data into a data processing pipeline and eventually into the model itself\n",
    "if classifier == \"NB\":\n",
    "    print(\"Training NB classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', MultinomialNB(fit_prior=fit_prior))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"D3\":\n",
    "    print(\"Training D3 classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"SVM\":\n",
    "    print(\"Training SVM classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', SVC(kernel='linear'))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"KNN\":\n",
    "    print(\"Training KNN classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors = 3))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_clf, open('./pickle/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GridSearchCV to search for best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.271405Z",
     "start_time": "2018-05-15T07:25:44.789019Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    # Grid Search\n",
    "    # Here, we are creating a list of parameters for which we would like to do performance tuning.\n",
    "    # All the parameters name start with the classifier name (remember the arbitrary name we gave).\n",
    "    # E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "    # NB parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__use_idf': (True, False)\n",
    "    }\n",
    "\n",
    "    # Next, we create an instance of the grid search by passing the classifier, parameters\n",
    "    # and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "    \n",
    "\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "\n",
    "    # To see the best mean score and the params, run the following code\n",
    "    gs_clf.best_score_\n",
    "    gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.280680Z",
     "start_time": "2018-05-15T07:26:20.274677Z"
    }
   },
   "source": [
    "# Save GSCV Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    pickle.dump(text_clf, open('./pickle/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.189731Z",
     "start_time": "2018-05-15T07:26:20.282676Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "Confusion matrix without GridSearch:\n",
      "[[ 435   50  670]\n",
      " [ 260   69  663]\n",
      " [1009  169 5297]]\n",
      "Mean without GridSearch: 0.6728137323126885\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model\")\n",
    "# Score and evaluate model on test data using model without hyperparameter tuning\n",
    "predicted = text_clf.predict(test_data)\n",
    "prediction_acc = np.mean(predicted == test_labels)\n",
    "print(\"Confusion matrix without GridSearch:\")\n",
    "print(metrics.confusion_matrix(test_labels, predicted))\n",
    "print(\"Mean without GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "\n",
    "# Score and evaluate model on test data using model WITH hyperparameter tuning\n",
    "if use_grid_search:\n",
    "    predicted = gs_clf.predict(test_data)\n",
    "    prediction_acc = np.mean(predicted == test_labels)\n",
    "    print(\"Confusion matrix with GridSearch:\")\n",
    "    print(metrics.confusion_matrix(test_labels, predicted))\n",
    "    print(\"Mean with GridSearch: \" + str(prediction_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data with inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestTickets = pd.read_csv(\n",
    "    './test data/testing_tickets.csv',\n",
    "    dtype=str\n",
    ")\n",
    "test_input_data = dfTestTickets[text_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "business_service ,\n",
      "19\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 3\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 4\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 5\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 6\n",
      "business_service ,\n",
      "45\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 7\n",
      "business_service ,\n",
      "50\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 8\n",
      "business_service ,\n",
      "61\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 9\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 10\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 11\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 12\n",
      "business_service ,\n",
      "67\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 13\n",
      "business_service ,\n",
      "1\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 14\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 15\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 16\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 17\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 18\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 19\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 20\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 21\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 22\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 23\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 24\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 25\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 26\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 27\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 28\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 29\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 30\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 31\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "text_clf_model = pickle.load(open('./pickle/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "prediction_input = text_clf_model.predict(test_input_data)\n",
    "\n",
    "i = 0\n",
    "for result in prediction_input:\n",
    "    print(\"No:\",i+2)\n",
    "    print(text_columns,\",\")\n",
    "    print(test_input_data.iloc[i])\n",
    "    print(\"-\"*30)\n",
    "    print(column_to_predict,\":\", result)\n",
    "    print(\"=\"*30)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    gs_clf_model = pickle.load(open('./pickle/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "    prediction_input = gs_clf_model.predict(test_input_data)\n",
    "\n",
    "    i = 0\n",
    "    for result in prediction_input:\n",
    "        print(\"No:\",i+2)\n",
    "        print(text_columns,\",\")\n",
    "        print(test_input_data.iloc[i])\n",
    "        print(\"-\"*30)\n",
    "        print(column_to_predict,\":\", result)\n",
    "        print(\"=\"*30)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.627104Z",
     "start_time": "2018-05-15T07:26:21.192694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting confusion matrix with 'seaborn' module\n",
    "# Use below line only with Jupyter Notebook\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "mat = confusion_matrix(test_labels, predicted)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set()\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(test_labels),\n",
    "            yticklabels=np.unique(test_labels))\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "# Save confusion matrix to outputs in Workbench\n",
    "# plt.savefig(os.path.join('.', 'outputs', 'confusion_matrix.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.684170Z",
     "start_time": "2018-05-15T07:26:21.631088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.38      0.30      1155\n",
      "           2       0.24      0.07      0.11       992\n",
      "           3       0.80      0.82      0.81      6475\n",
      "\n",
      "    accuracy                           0.67      8622\n",
      "   macro avg       0.43      0.42      0.41      8622\n",
      "weighted avg       0.66      0.67      0.66      8622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, predicted,\n",
    "                            target_names=np.unique(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
