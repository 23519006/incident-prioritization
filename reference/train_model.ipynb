{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:39:53.348806Z",
     "start_time": "2018-05-15T05:39:53.345777Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.937012Z",
     "start_time": "2018-05-15T07:25:29.984509Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from helpers import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "# Use the Azure Machine Learning data preparation package\n",
    "# from azureml.dataprep import package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.945018Z",
     "start_time": "2018-05-15T07:25:40.939016Z"
    }
   },
   "outputs": [],
   "source": [
    "column_to_predict = \"urgency\"\n",
    "# Supported datasets:\n",
    "# ticket_type\n",
    "# business_service\n",
    "# category\n",
    "# impact\n",
    "# urgency\n",
    "# sub_category1\n",
    "# sub_category2\n",
    "\n",
    "classifier = \"KNN\"  # Supported algorithms # \"SVM\" # \"NB\"\n",
    "use_grid_search = True  # grid search is used to find hyperparameters. Searching for hyperparameters is time consuming\n",
    "remove_stop_words = True  # removes stop words from processed text\n",
    "stop_words_lang = 'english'  # used with 'remove_stop_words' and defines language of stop words collection\n",
    "use_stemming = False  # word stemming using nltk\n",
    "fit_prior = True  # if use_stemming == True then it should be set to False ?? double check\n",
    "min_data_per_class = 1  # used to determine number of samples required for each class.Classes with less than that will be excluded from the dataset. default value is 1\n",
    "\n",
    "\n",
    "sampler = \"Over\"\n",
    "\n",
    "if classifier == \"KNN\":\n",
    "    use_grid_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.218329Z",
     "start_time": "2018-05-15T07:25:40.947014Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading dataset from dprep in Workbench    \n",
    "# dfTickets = package.run('AllTickets.dprep', dataflow_idx=0) \n",
    "\n",
    "# loading dataset from csv\n",
    "dfTickets = pd.read_csv(\n",
    "    './datasets/all_tickets.csv',\n",
    "    dtype=str\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select 'TEXT' column and remove poorly represented classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.321758Z",
     "start_time": "2018-05-15T07:25:41.220101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes before removing classes with less then 1 rows: 3\n",
      "Shape of dataset after removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes after removing classes with less then 1 rows: 3\n"
     ]
    }
   ],
   "source": [
    "text_columns = \"title\"  # \"title\" - text columns used for TF-IDF\n",
    "\n",
    "# Removing rows related to classes represented by low amount of data\n",
    "print(\"Shape of dataset before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(dfTickets.shape))\n",
    "print(\"Number of classes before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(len(np.unique(dfTickets[column_to_predict]))))\n",
    "bytag = dfTickets.groupby(column_to_predict).aggregate(np.count_nonzero)\n",
    "tags = bytag[bytag.body > min_data_per_class].index\n",
    "dfTickets = dfTickets[dfTickets[column_to_predict].isin(tags)]\n",
    "print(\n",
    "    \"Shape of dataset after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(dfTickets.shape)\n",
    ")\n",
    "print(\n",
    "    \"Number of classes after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(len(np.unique(dfTickets[column_to_predict])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and labels and split them to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.365785Z",
     "start_time": "2018-05-15T07:25:41.324755Z"
    }
   },
   "outputs": [],
   "source": [
    "labelData = dfTickets[column_to_predict]\n",
    "data = dfTickets[text_columns]\n",
    "\n",
    "# Split dataset into training and testing data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labelData, test_size=0.2\n",
    ")  # split data to train/test sets with 80:20 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.374133Z",
     "start_time": "2018-05-15T07:25:41.368126Z"
    }
   },
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:42.972298Z",
     "start_time": "2018-05-15T07:25:41.377130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 3680)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorizer\n",
    "if remove_stop_words:\n",
    "    count_vect = CountVectorizer(stop_words=stop_words_lang)\n",
    "elif use_stemming:\n",
    "    count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n",
    "else:\n",
    "    count_vect = CountVectorizer()\n",
    "\n",
    "vectorized_data = count_vect.fit_transform(train_data)\n",
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:43.026267Z",
     "start_time": "2018-05-15T07:25:42.975265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 3680)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "features = tfidf.fit_transform(vectorized_data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampler == \"Over\":\n",
    "    imbl_samp = RandomOverSampler()\n",
    "elif sampler == \"Under\":\n",
    "    imbl_samp = RandomUnderSampler()\n",
    "else:\n",
    "    imbl_samp = RandomOverSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipeline to preprocess data and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:44.786016Z",
     "start_time": "2018-05-15T07:25:43.028264Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN classifier\n"
     ]
    }
   ],
   "source": [
    "# Fitting the training data into a data processing pipeline and eventually into the model itself\n",
    "if classifier == \"NB\":\n",
    "    print(\"Training NB classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', MultinomialNB(fit_prior=fit_prior))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"D3\":\n",
    "    print(\"Training D3 classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"SVM\":\n",
    "    print(\"Training SVM classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', SVC(kernel='linear'))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"KNN\":\n",
    "    print(\"Training KNN classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors = 3))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_clf, open('./pickle/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GridSearchCV to search for best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.271405Z",
     "start_time": "2018-05-15T07:25:44.789019Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    # Grid Search\n",
    "    # Here, we are creating a list of parameters for which we would like to do performance tuning.\n",
    "    # All the parameters name start with the classifier name (remember the arbitrary name we gave).\n",
    "    # E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "    # NB parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__use_idf': (True, False)\n",
    "    }\n",
    "\n",
    "    # Next, we create an instance of the grid search by passing the classifier, parameters\n",
    "    # and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "    \n",
    "\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "\n",
    "    # To see the best mean score and the params, run the following code\n",
    "    gs_clf.best_score_\n",
    "    gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.280680Z",
     "start_time": "2018-05-15T07:26:20.274677Z"
    }
   },
   "source": [
    "# Save GSCV Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    pickle.dump(text_clf, open('./pickle/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.189731Z",
     "start_time": "2018-05-15T07:26:20.282676Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "Confusion matrix without GridSearch:\n",
      "[[ 520  237  417]\n",
      " [ 305  368  350]\n",
      " [ 467  382 5576]]\n",
      "Mean without GridSearch: 0.7497100440733009\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model\")\n",
    "# Score and evaluate model on test data using model without hyperparameter tuning\n",
    "predicted = text_clf.predict(test_data)\n",
    "prediction_acc = np.mean(predicted == test_labels)\n",
    "print(\"Confusion matrix without GridSearch:\")\n",
    "print(metrics.confusion_matrix(test_labels, predicted))\n",
    "print(\"Mean without GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "\n",
    "# Score and evaluate model on test data using model WITH hyperparameter tuning\n",
    "if use_grid_search:\n",
    "    predicted = gs_clf.predict(test_data)\n",
    "    prediction_acc = np.mean(predicted == test_labels)\n",
    "    print(\"Confusion matrix with GridSearch:\")\n",
    "    print(metrics.confusion_matrix(test_labels, predicted))\n",
    "    print(\"Mean with GridSearch: \" + str(prediction_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data with inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestTickets = pd.read_csv(\n",
    "    './test data/testing_tickets.csv',\n",
    "    dtype=str\n",
    ")\n",
    "test_input_data = dfTestTickets[text_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "title ,\n",
      "service unavailable\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 3\n",
      "title ,\n",
      "for us not working\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 4\n",
      "title ,\n",
      "not responding\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 5\n",
      "title ,\n",
      "performance issues and\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 6\n",
      "title ,\n",
      "connection down th floor\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 7\n",
      "title ,\n",
      "inaccessible\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 8\n",
      "title ,\n",
      "urgent cannot open files on\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 9\n",
      "title ,\n",
      "wireless network down\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 10\n",
      "title ,\n",
      "not accessible\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 11\n",
      "title ,\n",
      "certificate expired\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 12\n",
      "title ,\n",
      "oracle errors when logging\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 13\n",
      "title ,\n",
      "ad account change\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 14\n",
      "title ,\n",
      "conference codes needed\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 15\n",
      "title ,\n",
      "account for\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 16\n",
      "title ,\n",
      "conference line for\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 17\n",
      "title ,\n",
      "dial codes\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 18\n",
      "title ,\n",
      "dial details\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 19\n",
      "title ,\n",
      "provide new code\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 20\n",
      "title ,\n",
      "file\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 21\n",
      "title ,\n",
      "cannot access\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 22\n",
      "title ,\n",
      "unable to connect to\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 23\n",
      "title ,\n",
      "issue\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 24\n",
      "title ,\n",
      "investigate why open issue keeps occurring every day\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 25\n",
      "title ,\n",
      "error\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 26\n",
      "title ,\n",
      "open issue\n",
      "------------------------------\n",
      "urgency : 1\n",
      "==============================\n",
      "No: 27\n",
      "title ,\n",
      "is down can access the network\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 28\n",
      "title ,\n",
      "open\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n",
      "No: 29\n",
      "title ,\n",
      "provost issues\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 30\n",
      "title ,\n",
      "problems connecting to\n",
      "------------------------------\n",
      "urgency : 2\n",
      "==============================\n",
      "No: 31\n",
      "title ,\n",
      "access to\n",
      "------------------------------\n",
      "urgency : 3\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "text_clf_model = pickle.load(open('./pickle/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "prediction_input = text_clf_model.predict(test_input_data)\n",
    "\n",
    "i = 0\n",
    "for result in prediction_input:\n",
    "    print(\"No:\",i+2)\n",
    "    print(text_columns,\",\")\n",
    "    print(test_input_data.iloc[i])\n",
    "    print(\"-\"*30)\n",
    "    print(column_to_predict,\":\", result)\n",
    "    print(\"=\"*30)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    gs_clf_model = pickle.load(open('./pickle/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "    prediction_input = gs_clf_model.predict(test_input_data)\n",
    "\n",
    "    i = 0\n",
    "    for result in prediction_input:\n",
    "        print(\"No:\",i+2)\n",
    "        print(text_columns,\",\")\n",
    "        print(test_input_data.iloc[i])\n",
    "        print(\"-\"*30)\n",
    "        print(column_to_predict,\":\", result)\n",
    "        print(\"=\"*30)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.627104Z",
     "start_time": "2018-05-15T07:26:21.192694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting confusion matrix with 'seaborn' module\n",
    "# Use below line only with Jupyter Notebook\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "mat = confusion_matrix(test_labels, predicted)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set()\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(test_labels),\n",
    "            yticklabels=np.unique(test_labels))\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "# Save confusion matrix to outputs in Workbench\n",
    "# plt.savefig(os.path.join('.', 'outputs', 'confusion_matrix.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.684170Z",
     "start_time": "2018-05-15T07:26:21.631088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.44      0.42      1174\n",
      "           2       0.37      0.36      0.37      1023\n",
      "           3       0.88      0.87      0.87      6425\n",
      "\n",
      "    accuracy                           0.75      8622\n",
      "   macro avg       0.55      0.56      0.55      8622\n",
      "weighted avg       0.75      0.75      0.75      8622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, predicted,\n",
    "                            target_names=np.unique(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
