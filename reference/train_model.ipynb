{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:39:53.348806Z",
     "start_time": "2018-05-15T05:39:53.345777Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.937012Z",
     "start_time": "2018-05-15T07:25:29.984509Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from helpers import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "# Use the Azure Machine Learning data preparation package\n",
    "# from azureml.dataprep import package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:40.945018Z",
     "start_time": "2018-05-15T07:25:40.939016Z"
    }
   },
   "outputs": [],
   "source": [
    "column_to_predict = \"impact\"\n",
    "# Supported datasets:\n",
    "# ticket_type\n",
    "# business_service\n",
    "# category\n",
    "# impact\n",
    "# urgency\n",
    "# sub_category1\n",
    "# sub_category2\n",
    "\n",
    "classifier = \"SVM\"  # Supported algorithms # \"SVM\" # \"NB\"\n",
    "use_grid_search = True  # grid search is used to find hyperparameters. Searching for hyperparameters is time consuming\n",
    "remove_stop_words = True  # removes stop words from processed text\n",
    "stop_words_lang = 'english'  # used with 'remove_stop_words' and defines language of stop words collection\n",
    "use_stemming = False  # word stemming using nltk\n",
    "fit_prior = True  # if use_stemming == True then it should be set to False ?? double check\n",
    "min_data_per_class = 1  # used to determine number of samples required for each class.Classes with less than that will be excluded from the dataset. default value is 1\n",
    "\n",
    "\n",
    "sampler = \"Over\"\n",
    "\n",
    "if classifier == \"KNN\":\n",
    "    use_grid_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.218329Z",
     "start_time": "2018-05-15T07:25:40.947014Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading dataset from dprep in Workbench    \n",
    "# dfTickets = package.run('AllTickets.dprep', dataflow_idx=0) \n",
    "\n",
    "# loading dataset from csv\n",
    "dfTickets = pd.read_csv(\n",
    "    './datasets/all_tickets.csv',\n",
    "    dtype=str\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select 'TEXT' column and remove poorly represented classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.321758Z",
     "start_time": "2018-05-15T07:25:41.220101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes before removing classes with less then 1 rows: 3\n",
      "Shape of dataset after removing classes with less then 1 rows: (43107, 7)\n",
      "Number of classes after removing classes with less then 1 rows: 3\n"
     ]
    }
   ],
   "source": [
    "text_columns = \"business_service\"  # \"title\" - text columns used for TF-IDF\n",
    "\n",
    "# Removing rows related to classes represented by low amount of data\n",
    "print(\"Shape of dataset before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(dfTickets.shape))\n",
    "print(\"Number of classes before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(len(np.unique(dfTickets[column_to_predict]))))\n",
    "bytag = dfTickets.groupby(column_to_predict).aggregate(np.count_nonzero)\n",
    "tags = bytag[bytag.body > min_data_per_class].index\n",
    "dfTickets = dfTickets[dfTickets[column_to_predict].isin(tags)]\n",
    "print(\n",
    "    \"Shape of dataset after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(dfTickets.shape)\n",
    ")\n",
    "print(\n",
    "    \"Number of classes after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(len(np.unique(dfTickets[column_to_predict])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and labels and split them to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.365785Z",
     "start_time": "2018-05-15T07:25:41.324755Z"
    }
   },
   "outputs": [],
   "source": [
    "labelData = dfTickets[column_to_predict]\n",
    "data = dfTickets[text_columns]\n",
    "\n",
    "# Split dataset into training and testing data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labelData, test_size=0.2\n",
    ")  # split data to train/test sets with 80:20 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:41.374133Z",
     "start_time": "2018-05-15T07:25:41.368126Z"
    }
   },
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:42.972298Z",
     "start_time": "2018-05-15T07:25:41.377130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 90)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorizer\n",
    "if remove_stop_words:\n",
    "    count_vect = CountVectorizer(stop_words=stop_words_lang)\n",
    "elif use_stemming:\n",
    "    count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n",
    "else:\n",
    "    count_vect = CountVectorizer()\n",
    "\n",
    "vectorized_data = count_vect.fit_transform(train_data)\n",
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:43.026267Z",
     "start_time": "2018-05-15T07:25:42.975265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34485, 90)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "features = tfidf.fit_transform(vectorized_data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampler == \"Over\":\n",
    "    imbl_samp = RandomOverSampler()\n",
    "elif sampler == \"Under\":\n",
    "    imbl_samp = RandomUnderSampler()\n",
    "else:\n",
    "    imbl_samp = RandomOverSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipeline to preprocess data and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:25:44.786016Z",
     "start_time": "2018-05-15T07:25:43.028264Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM classifier\n"
     ]
    }
   ],
   "source": [
    "# Fitting the training data into a data processing pipeline and eventually into the model itself\n",
    "if classifier == \"NB\":\n",
    "    print(\"Training NB classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', MultinomialNB(fit_prior=fit_prior))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"D3\":\n",
    "    print(\"Training D3 classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"SVM\":\n",
    "    print(\"Training SVM classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', SVC(kernel='linear'))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)\n",
    "elif classifier == \"KNN\":\n",
    "    print(\"Training KNN classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('samp', imbl_samp),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors = 3))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_clf, open('./pickle/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GridSearchCV to search for best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.271405Z",
     "start_time": "2018-05-15T07:25:44.789019Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    # Grid Search\n",
    "    # Here, we are creating a list of parameters for which we would like to do performance tuning.\n",
    "    # All the parameters name start with the classifier name (remember the arbitrary name we gave).\n",
    "    # E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "    # NB parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__use_idf': (True, False)\n",
    "    }\n",
    "\n",
    "    # Next, we create an instance of the grid search by passing the classifier, parameters\n",
    "    # and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "    \n",
    "\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "\n",
    "    # To see the best mean score and the params, run the following code\n",
    "    gs_clf.best_score_\n",
    "    gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:20.280680Z",
     "start_time": "2018-05-15T07:26:20.274677Z"
    }
   },
   "source": [
    "# Save GSCV Model to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_grid_search:\n",
    "    pickle.dump(text_clf, open('./pickle/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.189731Z",
     "start_time": "2018-05-15T07:26:20.282676Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "Confusion matrix without GridSearch:\n",
      "[[  16   10   10]\n",
      " [ 479  889  742]\n",
      " [1655 1022 3799]]\n",
      "Mean without GridSearch: 0.5455810716771051\n",
      "Confusion matrix with GridSearch:\n",
      "[[  13   11   12]\n",
      " [ 409  904  797]\n",
      " [1322 1060 4094]]\n",
      "Mean with GridSearch: 0.5811876594757597\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model\")\n",
    "# Score and evaluate model on test data using model without hyperparameter tuning\n",
    "predicted = text_clf.predict(test_data)\n",
    "prediction_acc = np.mean(predicted == test_labels)\n",
    "print(\"Confusion matrix without GridSearch:\")\n",
    "print(metrics.confusion_matrix(test_labels, predicted))\n",
    "print(\"Mean without GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "\n",
    "# Score and evaluate model on test data using model WITH hyperparameter tuning\n",
    "if use_grid_search:\n",
    "    predicted = gs_clf.predict(test_data)\n",
    "    prediction_acc = np.mean(predicted == test_labels)\n",
    "    print(\"Confusion matrix with GridSearch:\")\n",
    "    print(metrics.confusion_matrix(test_labels, predicted))\n",
    "    print(\"Mean with GridSearch: \" + str(prediction_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data with inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestTickets = pd.read_csv(\n",
    "    './test data/testing_tickets.csv',\n",
    "    dtype=str\n",
    ")\n",
    "test_input_data = dfTestTickets[text_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "business_service ,\n",
      "19\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 3\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 4\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 5\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 6\n",
      "business_service ,\n",
      "45\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 7\n",
      "business_service ,\n",
      "50\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 8\n",
      "business_service ,\n",
      "61\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 9\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 10\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 11\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 12\n",
      "business_service ,\n",
      "67\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 13\n",
      "business_service ,\n",
      "1\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 14\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 15\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 16\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 17\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 18\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 19\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 20\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 21\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 22\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 23\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 24\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 25\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 26\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 27\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 28\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 29\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 30\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 31\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "text_clf_model = pickle.load(open('./pickle/text_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "prediction_input = text_clf_model.predict(test_input_data)\n",
    "\n",
    "i = 0\n",
    "for result in prediction_input:\n",
    "    print(\"No:\",i+2)\n",
    "    print(text_columns,\",\")\n",
    "    print(test_input_data.iloc[i])\n",
    "    print(\"-\"*30)\n",
    "    print(column_to_predict,\":\", result)\n",
    "    print(\"=\"*30)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 2\n",
      "business_service ,\n",
      "19\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 3\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 4\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 5\n",
      "business_service ,\n",
      "26\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 6\n",
      "business_service ,\n",
      "45\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 7\n",
      "business_service ,\n",
      "50\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 8\n",
      "business_service ,\n",
      "61\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 9\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 10\n",
      "business_service ,\n",
      "63\n",
      "------------------------------\n",
      "impact : 1\n",
      "==============================\n",
      "No: 11\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 12\n",
      "business_service ,\n",
      "67\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 13\n",
      "business_service ,\n",
      "1\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 14\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 15\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 16\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 17\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 18\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 19\n",
      "business_service ,\n",
      "2\n",
      "------------------------------\n",
      "impact : 3\n",
      "==============================\n",
      "No: 20\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 21\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 22\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 23\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 24\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 25\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 26\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 27\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 28\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 29\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 30\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n",
      "No: 31\n",
      "business_service ,\n",
      "65\n",
      "------------------------------\n",
      "impact : 2\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "if use_grid_search:\n",
    "    gs_clf_model = pickle.load(open('./pickle/gs_'+classifier+'_'+text_columns+'_'+column_to_predict+'_model.pickle',\"rb\"))\n",
    "    prediction_input = gs_clf_model.predict(test_input_data)\n",
    "\n",
    "    i = 0\n",
    "    for result in prediction_input:\n",
    "        print(\"No:\",i+2)\n",
    "        print(text_columns,\",\")\n",
    "        print(test_input_data.iloc[i])\n",
    "        print(\"-\"*30)\n",
    "        print(column_to_predict,\":\", result)\n",
    "        print(\"=\"*30)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.627104Z",
     "start_time": "2018-05-15T07:26:21.192694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting confusion matrix with 'seaborn' module\n",
    "# Use below line only with Jupyter Notebook\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "mat = confusion_matrix(test_labels, predicted)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set()\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(test_labels),\n",
    "            yticklabels=np.unique(test_labels))\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "# Save confusion matrix to outputs in Workbench\n",
    "# plt.savefig(os.path.join('.', 'outputs', 'confusion_matrix.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:26:21.684170Z",
     "start_time": "2018-05-15T07:26:21.631088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.01      0.36      0.01        36\n",
      "           2       0.46      0.43      0.44      2110\n",
      "           3       0.83      0.63      0.72      6476\n",
      "\n",
      "    accuracy                           0.58      8622\n",
      "   macro avg       0.43      0.47      0.39      8622\n",
      "weighted avg       0.74      0.58      0.65      8622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, predicted,\n",
    "                            target_names=np.unique(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
